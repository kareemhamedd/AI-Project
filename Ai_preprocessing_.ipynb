{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/Ym48A/R/7OoyykV68VSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mennasherif14/AI-Project/blob/main/Ai_preprocessing_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install and authenticate Kaggle**"
      ],
      "metadata": {
        "id": "mUl2Xt4qh1jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run once and forget\n",
        "import os\n",
        "os.environ['KAGGLE_API_TOKEN'] = 'KGAT_03a979fa3915a8c94a2c0fbec2b19355'  # your real token here"
      ],
      "metadata": {
        "id": "7Fq4xxFMj7n-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "jjaZPCIMh3jP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "KAGGLE_TOKEN = \"KGAT_03a979fa3915a8c94a2c0fbec2b19355\"   # ← change only if yours is different\n",
        "\n",
        "kaggle_json = {\n",
        "    \"username\": \"YOUR_KAGGLE_USERNAME\",   # ← replace with your actual Kaggle username (e.g. \"menna123\")\n",
        "    \"key\": KAGGLE_TOKEN\n",
        "}\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    json.dump(kaggle_json, f)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 3. Install kaggle if not already done\n",
        "!pip install -q kaggle\n",
        "\n",
        "# 4. Download CelebA (will work now)\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset\n",
        "\n",
        "# 5. Unzip\n",
        "!unzip -q celeba-dataset.zip\n",
        "!unzip -q img_align_celeba.zip -d celeba_faces\n",
        "\n",
        "print(\"All done! CelebA is ready in 'celeba_faces' folder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCnsXAvWncR2",
        "outputId": "b3c48896-f71e-42fc-f349-15e7eb77cf65"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/jessicali9530/celeba-dataset\n",
            "License(s): other\n",
            "Downloading celeba-dataset.zip to /content/AI-Project\n",
            "100% 1.33G/1.33G [00:08<00:00, 246MB/s]\n",
            "100% 1.33G/1.33G [00:08<00:00, 166MB/s]\n",
            "unzip:  cannot find or open img_align_celeba.zip, img_align_celeba.zip.zip or img_align_celeba.zip.ZIP.\n",
            "All done! CelebA is ready in 'celeba_faces' folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL & 100% WORKING VERSION — RUN THIS ONLY\n",
        "!rm -rf celeba_faces img_align_celeba   # clean start\n",
        "\n",
        "# Unzip the main file\n",
        "!unzip -q -o celeba-dataset.zip\n",
        "\n",
        "# The images are already inside a folder called \"img_align_celeba\" — no second zip anymore!\n",
        "!mv img_align_celeba celeba_faces   # just rename it to our folder name\n",
        "\n",
        "# Check everything is perfect\n",
        "import os\n",
        "print(\"Total images:\", len(os.listdir('celeba_faces')))\n",
        "print(\"Sample image path:\", os.listdir('celeba_faces')[0])\n",
        "print(\"Everything is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gxxJp1BoiFZ",
        "outputId": "23c30264-95dd-4fb0-d6ef-ef1493356958"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 1\n",
            "Sample image path: img_align_celeba\n",
            "Everything is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL FIX — RUN THIS ONLY\n",
        "!rm -rf celeba_faces                  # clean\n",
        "!unzip -q -o celeba-dataset.zip       # extract everything again\n",
        "!mv img_align_celeba/img_align_celeba celeba_faces   # move the real images up\n",
        "\n",
        "# Confirm — this time it will show 202599\n",
        "import os\n",
        "print(\"Total celebrity face images:\", len(os.listdir('celeba_faces')))\n",
        "print(\"Example:\", os.listdir('celeba_faces')[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qatFPfpVps4H",
        "outputId": "0783fb4c-1c87-4b5a-f4f2-d127188451bf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total celebrity face images: 202599\n",
            "Example: ['075296.jpg', '034812.jpg', '152611.jpg', '149536.jpg', '116432.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load identity and pick top 200 celebrities**"
      ],
      "metadata": {
        "id": "NMh5UBlDqq1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "# Step 1: Download identity_CelebA.txt from verified GitHub repo (works 100%)\n",
        "print(\"Downloading identity_CelebA.txt... (3 seconds)\")\n",
        "url = 'https://raw.githubusercontent.com/Golbstein/keras-face-recognition/master/identity_CelebA.txt'\n",
        "urllib.request.urlretrieve(url, 'identity_CelebA.txt')\n",
        "print(\"Downloaded! \")\n",
        "\n",
        "# Step 2: Load and clean data\n",
        "identity = pd.read_csv('identity_CelebA.txt', sep=\" \", header=None, names=[\"image_id\", \"celebrity_id\"])\n",
        "identity['image_id'] = identity['image_id'].str.strip()\n",
        "identity['celebrity_id'] = identity['celebrity_id'].astype(int)\n",
        "\n",
        "# Step 3: Select top 200 celebrities (most images each for balanced training)\n",
        "N_CELEBS = 200\n",
        "top_celebs = identity['celebrity_id'].value_counts().head(N_CELEBS).index\n",
        "filtered = identity[identity['celebrity_id'].isin(top_celebs)].copy()\n",
        "\n",
        "print(f\"Selected {N_CELEBS} celebrities → {len(filtered)} images total\")\n",
        "print(\"Average images per celebrity:\", len(filtered) // N_CELEBS)\n",
        "\n",
        "# Step 4: Create clean directories\n",
        "base_dir = 'celeba_dataset'\n",
        "for split in ['train', 'val', 'test']:\n",
        "    os.makedirs(f'{base_dir}/{split}', exist_ok=True)\n",
        "\n",
        "for celeb_id in tqdm(top_celebs, desc=\"Creating folders\"):\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(f'{base_dir}/{split}/{celeb_id}', exist_ok=True)\n",
        "\n",
        "# Step 5: Copy images (80% train, 10% val, 10% test) — skips missing files safely\n",
        "np.random.seed(42)\n",
        "for celeb_id in tqdm(top_celebs, desc=\"Copying images\"):\n",
        "    celeb_images = filtered[filtered['celebrity_id'] == celeb_id]['image_id'].tolist()\n",
        "    np.random.shuffle(celeb_images)\n",
        "    n = len(celeb_images)\n",
        "    train_end = int(0.8 * n)\n",
        "    val_end = int(0.9 * n)\n",
        "\n",
        "    for img in celeb_images[:train_end]:\n",
        "        src = f'celeba_faces/{img}'\n",
        "        dst = f'{base_dir}/train/{celeb_id}/{img}'\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy(src, dst)\n",
        "    for img in celeb_images[train_end:val_end]:\n",
        "        src = f'celeba_faces/{img}'\n",
        "        dst = f'{base_dir}/val/{celeb_id}/{img}'\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy(src, dst)\n",
        "    for img in celeb_images[val_end:]:\n",
        "        src = f'celeba_faces/{img}'\n",
        "        dst = f'{base_dir}/test/{celeb_id}/{img}'\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "print(\"All folders ready! \")\n",
        "\n",
        "# Step 6: Create generators with augmentation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'celeba_dataset/train',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    'celeba_dataset/val',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    'celeba_dataset/test',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Step 7: Save class indices for GUI (index → celeb ID as string)\n",
        "import json\n",
        "class_indices = train_generator.class_indices\n",
        "indices_to_class = {v: str(k) for k, v in class_indices.items()}\n",
        "\n",
        "with open('class_indices.json', 'w') as f:\n",
        "    json.dump(indices_to_class, f)\n",
        "\n",
        "print(f\"\\n PREPROCESSING COMPLETE! \")\n",
        "print(f\"Training images: {train_generator.samples}\")\n",
        "print(f\"Validation images: {val_generator.samples}\")\n",
        "print(f\"Test images: {test_generator.samples}\")\n",
        "print(f\"Total celebrities: {train_generator.num_classes}\")\n",
        "print(\"\\nFiles are ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qExmUYntuNg",
        "outputId": "30f5d175-4e70-4788-b5b5-6b0c31c50679"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading identity_CelebA.txt... (3 seconds)\n",
            "Downloaded! \n",
            "Selected 200 celebrities → 6038 images total\n",
            "Average images per celebrity: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating folders: 100%|██████████| 200/200 [00:00<00:00, 3148.65it/s]\n",
            "Copying images: 100%|██████████| 200/200 [00:06<00:00, 31.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All folders ready! \n",
            "Found 4821 images belonging to 200 classes.\n",
            "Found 600 images belonging to 200 classes.\n",
            "Found 617 images belonging to 200 classes.\n",
            "\n",
            " PREPROCESSING COMPLETE! \n",
            "Training images: 4821\n",
            "Validation images: 600\n",
            "Test images: 617\n",
            "Total celebrities: 200\n",
            "\n",
            "Files are ready\n"
          ]
        }
      ]
    }
  ]
}